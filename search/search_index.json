{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast, Safe & Simple Asynchronous Task Queues Written In Pure Python Home # Sergeant is a library that was written at Intsights after failing to use celery at high scale. Our infrastructure had peaks of more than 100k tasks a second and thousands of workers. Celery found to be slow and unstable for that scale. Sergeant as opposed to celery is much simpler and easier. Instead of supporting a wide range of backends and scenarios, this library provides a simple and intuitive interface to implement a queue-based asynchronous workers architecture. The two supported backends are redis and mongo . Redis as an in-memory database, is recommended for high throughput environments, where performance is the top priority. Mongo is recommended for stable and consistent systems where you need tasks to be kept persistently. The decision not to support AMQP based backends was decided in order to leave this library as simple as possible and to leave the ability to build more complex mechanisms to the user. The library uses some latest python features such as dataclasses and f-strings , which makes this library to be compatible only with Python 3.7+ . The library supports multiple serializers and compressions. Each task, prior to being pushed into the queue, should be serialized, and optionally compressed. The supported serializers are pickle , json and msgpack . One should not switch the default serializer pickle unless it has some limitations such as security concerns, or programming languages portability issues. You should remember the each serializer supported different data types. While pickle is not portable, it support much more types than any other serializer. When executing tasks with parameters that are not supported by json or msgpack, you should serialize/deserialize manually, or use pickle. As for the compressions, this library support multiple built-in compressions algorithms that are supported natively by Python. zlib , gzip , bzip2 and lzma . The default compressor is None which does not compress at all. One should choose a different compressor to reduce the amount of RAM (when using Redis) and storage (when using mongo) if it has a huge amount of tasks being stored when the system is at peak capacity. The library tasks execution supported mechanisms are multiprocessing and threading . This means that each task might be executed, according to the choice of the implementor, in a different thread, or different process. The concurrency level is configurable. The library supports two watchdogs mechanisms to evade situations where the tasks are running for a longer time than expected. The first is a process killer. This type of watchdog spawns a separated process which waits for start and stop signals to make sure the tasks did not exceed the specified maximum timeout. When using a threading executor, a thread killer is being spawned. Due to the nature of threads in python, this killer has only one ability, and it is to raise an Exception in the context of the watched thread. In situations where the thread locks the GIL, like executing an infinite regex, this killer will fail at its job. Choosing a threaded executor should be taken with caution. The main process which takes care of the worker is the supervisor . The supervisor spawns the child workers, each one in a different process, and waits for their execution to complete. Once completed, the supervisor spawns another child in place of the worker. The supervisor also takes care of an out of bound workers. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"#home","text":"Sergeant is a library that was written at Intsights after failing to use celery at high scale. Our infrastructure had peaks of more than 100k tasks a second and thousands of workers. Celery found to be slow and unstable for that scale. Sergeant as opposed to celery is much simpler and easier. Instead of supporting a wide range of backends and scenarios, this library provides a simple and intuitive interface to implement a queue-based asynchronous workers architecture. The two supported backends are redis and mongo . Redis as an in-memory database, is recommended for high throughput environments, where performance is the top priority. Mongo is recommended for stable and consistent systems where you need tasks to be kept persistently. The decision not to support AMQP based backends was decided in order to leave this library as simple as possible and to leave the ability to build more complex mechanisms to the user. The library uses some latest python features such as dataclasses and f-strings , which makes this library to be compatible only with Python 3.7+ . The library supports multiple serializers and compressions. Each task, prior to being pushed into the queue, should be serialized, and optionally compressed. The supported serializers are pickle , json and msgpack . One should not switch the default serializer pickle unless it has some limitations such as security concerns, or programming languages portability issues. You should remember the each serializer supported different data types. While pickle is not portable, it support much more types than any other serializer. When executing tasks with parameters that are not supported by json or msgpack, you should serialize/deserialize manually, or use pickle. As for the compressions, this library support multiple built-in compressions algorithms that are supported natively by Python. zlib , gzip , bzip2 and lzma . The default compressor is None which does not compress at all. One should choose a different compressor to reduce the amount of RAM (when using Redis) and storage (when using mongo) if it has a huge amount of tasks being stored when the system is at peak capacity. The library tasks execution supported mechanisms are multiprocessing and threading . This means that each task might be executed, according to the choice of the implementor, in a different thread, or different process. The concurrency level is configurable. The library supports two watchdogs mechanisms to evade situations where the tasks are running for a longer time than expected. The first is a process killer. This type of watchdog spawns a separated process which waits for start and stop signals to make sure the tasks did not exceed the specified maximum timeout. When using a threading executor, a thread killer is being spawned. Due to the nature of threads in python, this killer has only one ability, and it is to raise an Exception in the context of the watched thread. In situations where the thread locks the GIL, like executing an infinite regex, this killer will fail at its job. Choosing a threaded executor should be taken with caution. The main process which takes care of the worker is the supervisor . The supervisor spawns the child workers, each one in a different process, and waits for their execution to complete. Once completed, the supervisor spawns another child in place of the worker. The supervisor also takes care of an out of bound workers. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"supervisor/","text":"Supervisor # The supervisor is the main application that take the Worker class and supervise it. Command Line # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 python3 -m sergeant.supervisor --helpusage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place. concurrent-workers - How many subprocesses the supervisor should spawn and supervise. worker-module - The worker module in a dotted notation path. worker-class - The worker class name inside the module file - usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can use before the supervisor terminates it and respawn a new one instead. Examples # Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-worker = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 When a worker reaches its end of life - at the moment it finished max_tasks_per_run tasks, it will exit, and a new worker will be created by the supervisor.","title":"Supervisor"},{"location":"supervisor/#supervisor","text":"The supervisor is the main application that take the Worker class and supervise it.","title":"Supervisor"},{"location":"supervisor/#command-line","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 python3 -m sergeant.supervisor --helpusage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place. concurrent-workers - How many subprocesses the supervisor should spawn and supervise. worker-module - The worker module in a dotted notation path. worker-class - The worker class name inside the module file - usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can use before the supervisor terminates it and respawn a new one instead.","title":"Command Line"},{"location":"supervisor/#examples","text":"Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-worker = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 When a worker reaches its end of life - at the moment it finished max_tasks_per_run tasks, it will exit, and a new worker will be created by the supervisor.","title":"Examples"},{"location":"examples/single_producer_consumer/","text":"Single Producer-Consumer # We will start with a simple Consumer-Producer pattern to understand how it feels to work with sergeant Graph # graph LR Producer -.->|Push Tasks| Broker{{Broker}} Broker -.->|Pull Tasks| Consumer Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sergeant import logging class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: {task[\"kwargs\"]} ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_task_queue () worker . purge_tasks () worker . apply_async_one ( kwargs = { 'some_parameter' : 'one' , }, ) Explanation # Consumer # At the class definition, we inherit from sergeant.worker.Worker to gain all the Worker class abilities. In order for this worker to be able to produce and consume tasks, we need to define config attribute and work method. Defining the config class attribute, allow us to config our worker's abilities. The worker config is a dataclass named sergeant.config.WorkerConfig which has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name, and the queue name within the broker, and the second is the connector . The connector is the device that is responsible to talk with the broker. In this example we define a test_worker worker with a redis connector. logging helps to configure the logger. For each consumed task, the work method is invoked. The parameters are passed inside the task argument as the key kwargs . Producer # The producer first loads the consumer module. The reason for that is that once instantiating a Worker , you can use its configuration. The producer uses the Worker instance so it will have a connection to the broker. Later, we call to init_task_queue so we will create the connection to the task queue inside the Worker instance. We call purge_tasks to assure no leftover tasks exist in the queue. apply_async_one is the function that compose a task object, and pushes it to the queue. Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#single-producer-consumer","text":"We will start with a simple Consumer-Producer pattern to understand how it feels to work with sergeant","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#graph","text":"graph LR Producer -.->|Push Tasks| Broker{{Broker}} Broker -.->|Pull Tasks| Consumer","title":"Graph"},{"location":"examples/single_producer_consumer/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sergeant import logging class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: {task[\"kwargs\"]} ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_task_queue () worker . purge_tasks () worker . apply_async_one ( kwargs = { 'some_parameter' : 'one' , }, )","title":"Code"},{"location":"examples/single_producer_consumer/#explanation","text":"","title":"Explanation"},{"location":"examples/single_producer_consumer/#consumer","text":"At the class definition, we inherit from sergeant.worker.Worker to gain all the Worker class abilities. In order for this worker to be able to produce and consume tasks, we need to define config attribute and work method. Defining the config class attribute, allow us to config our worker's abilities. The worker config is a dataclass named sergeant.config.WorkerConfig which has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name, and the queue name within the broker, and the second is the connector . The connector is the device that is responsible to talk with the broker. In this example we define a test_worker worker with a redis connector. logging helps to configure the logger. For each consumed task, the work method is invoked. The parameters are passed inside the task argument as the key kwargs .","title":"Consumer"},{"location":"examples/single_producer_consumer/#producer","text":"The producer first loads the consumer module. The reason for that is that once instantiating a Worker , you can use its configuration. The producer uses the Worker instance so it will have a connection to the broker. Later, we call to init_task_queue so we will create the connection to the task queue inside the Worker instance. We call purge_tasks to assure no leftover tasks exist in the queue. apply_async_one is the function that compose a task object, and pushes it to the queue.","title":"Producer"},{"location":"examples/single_producer_consumer/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"examples/worker_with_apm/","text":"Worker With APM - ElasticAPM # This example demonstrates how to integrate with an APM solution. In this case, ElasticAPM . Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_task_queue () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . apply_async_one ( kwargs = { 'url' : 'https://www.intsights.com/' , }, ) if __name__ == '__main__' : main () Explanation # In order this integrate an APM solution, we implemented initialize , finalize , pre_work and post_work . initialize - Since this function will run once per the whole worker lifetime, the initialization should happen here. This is where we declare the elasticapm.Client . pre_work - This function will run once per task, prior to its execution. This is where we will start a transaction. post_work - This function will run once per task, after its execution. This is where we will end the transaction. This is also where we will try to capture the exception, if any. finalize - This function will run once per the whole worker lifetime. This is where we will do cleanups. In this case, we will implicitly clean the apm client with a call to close . It is important to mention that this example can be easily extended to any other APM solutions such as jaeger and more. Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#worker-with-apm-elasticapm","text":"This example demonstrates how to integrate with an APM solution. In this case, ElasticAPM .","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_task_queue () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . apply_async_one ( kwargs = { 'url' : 'https://www.intsights.com/' , }, ) if __name__ == '__main__' : main ()","title":"Code"},{"location":"examples/worker_with_apm/#explanation","text":"In order this integrate an APM solution, we implemented initialize , finalize , pre_work and post_work . initialize - Since this function will run once per the whole worker lifetime, the initialization should happen here. This is where we declare the elasticapm.Client . pre_work - This function will run once per task, prior to its execution. This is where we will start a transaction. post_work - This function will run once per task, after its execution. This is where we will end the transaction. This is also where we will try to capture the exception, if any. finalize - This function will run once per the whole worker lifetime. This is where we will do cleanups. In this case, we will implicitly clean the apm client with a call to close . It is important to mention that this example can be easily extended to any other APM solutions such as jaeger and more.","title":"Explanation"},{"location":"examples/worker_with_apm/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"worker/config/connector/","text":"Worker Config - connector # The connector parameter controlls the connection to the broker. Definition # 1 2 3 4 @dataclasses . dataclass class Connector : type : str params : typing . Dict [ str , typing . Any ] The type parameter defines the type of the connector. The library currently supports the following types: redis - A single redis instance which holds the tasks. redis_cluster - Multiple redis instances that are not cluster-connected. The library manages the distribution of tasks on the client side by shuffling the list of connections and push/pull from each of them at different times. mongo - Using a mongodb server to hold the tasks. This is a great option for persistent tasks. The param parameter is being passed to the connector directly as **kwargs . Examples # redis 1 2 3 4 5 6 7 8 9 sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ) redis_cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis_cluster' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo 1 2 3 4 5 6 sergeant . config . Connector ( type = 'mongo' , params = { 'mongodb_uri' : 'mongodb://localhost:27017/' , }, )","title":"connector"},{"location":"worker/config/connector/#worker-config-connector","text":"The connector parameter controlls the connection to the broker.","title":"Worker Config - connector"},{"location":"worker/config/connector/#definition","text":"1 2 3 4 @dataclasses . dataclass class Connector : type : str params : typing . Dict [ str , typing . Any ] The type parameter defines the type of the connector. The library currently supports the following types: redis - A single redis instance which holds the tasks. redis_cluster - Multiple redis instances that are not cluster-connected. The library manages the distribution of tasks on the client side by shuffling the list of connections and push/pull from each of them at different times. mongo - Using a mongodb server to hold the tasks. This is a great option for persistent tasks. The param parameter is being passed to the connector directly as **kwargs .","title":"Definition"},{"location":"worker/config/connector/#examples","text":"redis 1 2 3 4 5 6 7 8 9 sergeant . config . Connector ( type = 'redis' , params = { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ) redis_cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis_cluster' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo 1 2 3 4 5 6 sergeant . config . Connector ( type = 'mongo' , params = { 'mongodb_uri' : 'mongodb://localhost:27017/' , }, )","title":"Examples"},{"location":"worker/config/encoder/","text":"Worker Config - encoder # The encoder parameter controls the encoder which is responsible for the tasks compression and serialization. Definition # 1 2 3 4 @dataclasses . dataclass class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter defines the type of the compressor. Each task prior to being pushed to the queue is going through a compressor. The usage of compressor can help reducing the storage/memory of the broker. Tasks with a lot of parameters/data can take a lot of memory and might fill the memory/storage quickly. Using a compressor would impact the performance of task pushing/pulling due to the compression algorithm being a CPU intensive operation. The following compressors are available: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer parameter defines the type of the serializer. Each task prior to being pushed to the queue should be serialized so the broker could save it as a byte array. Choosing the right serialization algorithm is important due to some limitations of each serialization algorithm. The following serializers are available: pickle [default]: pros: fast, native, many supported data types. cons: insecure (allows to run arbitrary code), non-portable, might mislead to think that the parameters were serialized correctly but deserializing them would end with a broken object. json pros: portable, secure. cons: relatively slow, few supported data types. msgpack pros: fast, portable, secure. cons: few supported data types. One can make any combination of compressor and serializer that suit your needs. Examples # default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"encoder"},{"location":"worker/config/encoder/#worker-config-encoder","text":"The encoder parameter controls the encoder which is responsible for the tasks compression and serialization.","title":"Worker Config - encoder"},{"location":"worker/config/encoder/#definition","text":"1 2 3 4 @dataclasses . dataclass class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter defines the type of the compressor. Each task prior to being pushed to the queue is going through a compressor. The usage of compressor can help reducing the storage/memory of the broker. Tasks with a lot of parameters/data can take a lot of memory and might fill the memory/storage quickly. Using a compressor would impact the performance of task pushing/pulling due to the compression algorithm being a CPU intensive operation. The following compressors are available: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer parameter defines the type of the serializer. Each task prior to being pushed to the queue should be serialized so the broker could save it as a byte array. Choosing the right serialization algorithm is important due to some limitations of each serialization algorithm. The following serializers are available: pickle [default]: pros: fast, native, many supported data types. cons: insecure (allows to run arbitrary code), non-portable, might mislead to think that the parameters were serialized correctly but deserializing them would end with a broken object. json pros: portable, secure. cons: relatively slow, few supported data types. msgpack pros: fast, portable, secure. cons: few supported data types. One can make any combination of compressor and serializer that suit your needs.","title":"Definition"},{"location":"worker/config/encoder/#examples","text":"default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"Examples"},{"location":"worker/config/executor/","text":"Worker Config - executor # The executor parameter controls which executor to use when executing tasks. Definition # 1 2 3 4 @dataclasses . dataclass class Executor : type : str = 'serial' number_of_threads : int = 1 The executor parameter defines the type of the tasks execution. The following executor types are available: serial [default] - Serial executor takes the bulk of the tasks that were pulled from the broker, and executes each one of them one by one. This executor means there is no parallelism within the process. threaded - Threaded executor takes the bulk of the tasks that were pulled from the broker, and executes them within a thread pool. This executor means that on the top of the process, there is parallelism between different threads. number_of_threads parameter is relevant only with threaded executor. It is very important to choose the right executor type. There are more consequences than one might think for choosing incorrect executor. When using the serial executor, the only concurrency you will get will be the amount of workers that were spawned by the supervisor . It is a very stable executor due to its nature. It takes a task after task, and runs it in a loop. The threaded executor take a bunch of tasks and runs them inside a thread pool. The big concern here is the inability to stop an execution of a thread in Python. A thread that would lock the GIL would make the worker inaccesible. The worker would be in a stuck mode. There are two different killers for each of the executors. When using the serial executor, the sergeant.killer.process is being used. This type of killer can kill the process entirely on situations where the process became stuck. When using the threaded executor, the sergeant.killer.thread is being used. This type of killer tries to raise an exception inside the stuck thread. If the thread holds the GIL, or is not switching its current execution line, it would stuck forever. A call for time.sleep(100) for example would not raise an exception until the sleep would end. The reason not to use signals here as we do in sergeant.killer.process is that we do not want to interrupt all the threads, and this would cause to drop multiple tasks. Examples # serial 1 2 3 sergeant . config . Executor ( type = 'serial' , ) threaded 1 2 3 4 sergeant . config . Executor ( type = 'threaded' , number_of_threads = 10 , )","title":"executor"},{"location":"worker/config/executor/#worker-config-executor","text":"The executor parameter controls which executor to use when executing tasks.","title":"Worker Config - executor"},{"location":"worker/config/executor/#definition","text":"1 2 3 4 @dataclasses . dataclass class Executor : type : str = 'serial' number_of_threads : int = 1 The executor parameter defines the type of the tasks execution. The following executor types are available: serial [default] - Serial executor takes the bulk of the tasks that were pulled from the broker, and executes each one of them one by one. This executor means there is no parallelism within the process. threaded - Threaded executor takes the bulk of the tasks that were pulled from the broker, and executes them within a thread pool. This executor means that on the top of the process, there is parallelism between different threads. number_of_threads parameter is relevant only with threaded executor. It is very important to choose the right executor type. There are more consequences than one might think for choosing incorrect executor. When using the serial executor, the only concurrency you will get will be the amount of workers that were spawned by the supervisor . It is a very stable executor due to its nature. It takes a task after task, and runs it in a loop. The threaded executor take a bunch of tasks and runs them inside a thread pool. The big concern here is the inability to stop an execution of a thread in Python. A thread that would lock the GIL would make the worker inaccesible. The worker would be in a stuck mode. There are two different killers for each of the executors. When using the serial executor, the sergeant.killer.process is being used. This type of killer can kill the process entirely on situations where the process became stuck. When using the threaded executor, the sergeant.killer.thread is being used. This type of killer tries to raise an exception inside the stuck thread. If the thread holds the GIL, or is not switching its current execution line, it would stuck forever. A call for time.sleep(100) for example would not raise an exception until the sleep would end. The reason not to use signals here as we do in sergeant.killer.process is that we do not want to interrupt all the threads, and this would cause to drop multiple tasks.","title":"Definition"},{"location":"worker/config/executor/#examples","text":"serial 1 2 3 sergeant . config . Executor ( type = 'serial' , ) threaded 1 2 3 4 sergeant . config . Executor ( type = 'threaded' , number_of_threads = 10 , )","title":"Examples"},{"location":"worker/config/logging/","text":"Worker Config - timeouts # The logging parameter controls the logger of the worker. Definition # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @dataclasses . dataclass class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) The following configurations are available: level [logging.ERROR] - The logging.level of the logger. Can be one of the available levels. log_to_stdout [False] - Whether the logger should log to stdout. events - On which events the logger should log. on_success [False] - Every time a task has finished successfully. on_failure [True] - Every time a task has failed. on_timeout [True] - Every time a task timed out. on_retry [True] - Every time a task asked for a retry. on_max_retries [True] - Every time a task asked for a retry beyond the maximum number of retries. on_requeue [True] - Every time a task asked for a requeue. handlers - List of handlers [logging.Handler] to attach to the logging.Logger object. Examples # STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"logging"},{"location":"worker/config/logging/#worker-config-timeouts","text":"The logging parameter controls the logger of the worker.","title":"Worker Config - timeouts"},{"location":"worker/config/logging/#definition","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @dataclasses . dataclass class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) The following configurations are available: level [logging.ERROR] - The logging.level of the logger. Can be one of the available levels. log_to_stdout [False] - Whether the logger should log to stdout. events - On which events the logger should log. on_success [False] - Every time a task has finished successfully. on_failure [True] - Every time a task has failed. on_timeout [True] - Every time a task timed out. on_retry [True] - Every time a task asked for a retry. on_max_retries [True] - Every time a task asked for a retry beyond the maximum number of retries. on_requeue [True] - Every time a task asked for a requeue. handlers - List of handlers [logging.Handler] to attach to the logging.Logger object.","title":"Definition"},{"location":"worker/config/logging/#examples","text":"STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"Examples"},{"location":"worker/config/max_retries/","text":"Worker Config - max_retries # max_retries defines how many retries the worker can invoke before the task will be dumped. This number should be int > 0 . Using this parameter is encouraged for tasks that retry exist within them. retry method is a function that sends back the current task to the queue after increasing the internal run_count . When the run_count reaches the max_retries number, calling retry again would result in an exception. Tasks that you don't want to retry them forever might configure this parameter. A value of 0 means you can retry infinitely. Definition # 1 max_retries : int = 0","title":"max_retries"},{"location":"worker/config/max_retries/#worker-config-max_retries","text":"max_retries defines how many retries the worker can invoke before the task will be dumped. This number should be int > 0 . Using this parameter is encouraged for tasks that retry exist within them. retry method is a function that sends back the current task to the queue after increasing the internal run_count . When the run_count reaches the max_retries number, calling retry again would result in an exception. Tasks that you don't want to retry them forever might configure this parameter. A value of 0 means you can retry infinitely.","title":"Worker Config - max_retries"},{"location":"worker/config/max_retries/#definition","text":"1 max_retries : int = 0","title":"Definition"},{"location":"worker/config/max_tasks_per_run/","text":"Worker Config - max_tasks_per_run # max_tasks_per_run defines how many tasks the worker should consume before killing it self and spawning another worker instead. This number should be int > 0 . The usage of this parameter is encouraged even if you think your worker must respawn itself ever. Memory leak situations might occuer from time to time without any simptoms. Declaring this parameter, might keep your worker healthy over time. A low number here is discouraged unless neccessary. Low number might cause the worker to die frequently and the overhead of respawning might be felt. A value of 0 means worker should never die. Definition # 1 max_tasks_per_run : int = 0","title":"max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#worker-config-max_tasks_per_run","text":"max_tasks_per_run defines how many tasks the worker should consume before killing it self and spawning another worker instead. This number should be int > 0 . The usage of this parameter is encouraged even if you think your worker must respawn itself ever. Memory leak situations might occuer from time to time without any simptoms. Declaring this parameter, might keep your worker healthy over time. A low number here is discouraged unless neccessary. Low number might cause the worker to die frequently and the overhead of respawning might be felt. A value of 0 means worker should never die.","title":"Worker Config - max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#definition","text":"1 max_tasks_per_run : int = 0","title":"Definition"},{"location":"worker/config/name/","text":"Worker Config - name # The name parameter an is important parameter inside the architecture. This parameter is mandatory. The usage of this parameter is crucial for the worker to be able to consume tasks. This parameter eventually becomes the queue name inside the broker. The producer should usu that name to invoke the corresponding worker. Using the same worker name with the same broker, would make the two workers share the same queue and eventually consume each other tasks. Definition # 1 name : str","title":"name"},{"location":"worker/config/name/#worker-config-name","text":"The name parameter an is important parameter inside the architecture. This parameter is mandatory. The usage of this parameter is crucial for the worker to be able to consume tasks. This parameter eventually becomes the queue name inside the broker. The producer should usu that name to invoke the corresponding worker. Using the same worker name with the same broker, would make the two workers share the same queue and eventually consume each other tasks.","title":"Worker Config - name"},{"location":"worker/config/name/#definition","text":"1 name : str","title":"Definition"},{"location":"worker/config/tasks_per_transaction/","text":"Worker Config - tasks_per_transaction # tasks_per_transaction defines how many tasks the worker will pull from the broker on each transaction. The worker loop works as follow: Worker pulls tasks from the broker. The worker tries to pull tasks_per_transaction tasks in one command. The worker executes each of the tasks one after one. At the end of executing all the pulled tasks, repeat phase 1. The reason the worker works in this way is to reduce the load on the broker. Pulling a bulk of 100 tasks is much easier than pulling a single task 100 times. The reason not to do that is to allow better distributions of tasks. Think about the situation that you have a task, that its execution takes 1 minute. Pulling 100 tasks would repeat every 100 minutes. Imagine running 2 workers to consume this task queue, and a producer that pushed only 100 tasks. The first worker to pull from the broker would consume all the tasks and leave nothing to the other worker. Think about another situation where you have a task that might take 1 second to 1 minute to complete. The producer would push for example 100 tasks, each worker would pull 50 tasks. The first worker finished its tasks within 1 minute for example and the second worker randomlly pulled longer to execute tasks. The first worker would starve for tasks while the other exhaust. This parameter should be chosen wisely. When you have a lot of tasks and each task is short, feel free to raise the number of tasks per transaction. When you have long tasks, leave this number as 1 to allow better tasks distribution. Definition # 1 tasks_per_transaction : int = 1","title":"tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#worker-config-tasks_per_transaction","text":"tasks_per_transaction defines how many tasks the worker will pull from the broker on each transaction. The worker loop works as follow: Worker pulls tasks from the broker. The worker tries to pull tasks_per_transaction tasks in one command. The worker executes each of the tasks one after one. At the end of executing all the pulled tasks, repeat phase 1. The reason the worker works in this way is to reduce the load on the broker. Pulling a bulk of 100 tasks is much easier than pulling a single task 100 times. The reason not to do that is to allow better distributions of tasks. Think about the situation that you have a task, that its execution takes 1 minute. Pulling 100 tasks would repeat every 100 minutes. Imagine running 2 workers to consume this task queue, and a producer that pushed only 100 tasks. The first worker to pull from the broker would consume all the tasks and leave nothing to the other worker. Think about another situation where you have a task that might take 1 second to 1 minute to complete. The producer would push for example 100 tasks, each worker would pull 50 tasks. The first worker finished its tasks within 1 minute for example and the second worker randomlly pulled longer to execute tasks. The first worker would starve for tasks while the other exhaust. This parameter should be chosen wisely. When you have a lot of tasks and each task is short, feel free to raise the number of tasks per transaction. When you have long tasks, leave this number as 1 to allow better tasks distribution.","title":"Worker Config - tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#definition","text":"1 tasks_per_transaction : int = 1","title":"Definition"},{"location":"worker/config/timeouts/","text":"Worker Config - timeouts # The timeouts parameter controls the killer timeouts for the worker. Definition # 1 2 3 4 5 @dataclasses . dataclass class Timeouts : soft_timeout : float = 0.0 hard_timeout : float = 0.0 critical_timeout : float = 0.0 The timeouts parameter defines how much time the process should run before the killer must try to kill it. The following timeouts can be configured: soft_timeout [both] - On serial executor, by the time this timeout is reached, a SIGINT would be sent to the worker. On threaded worker, an exception would be raised inside the thread. hard_timeout [serial] - By the time this timeout is reached, a SIGABRT would be sent to the worker. critical_timeout [serial] - By the time this timeout is reached, a SIGKILL would be sent to the worker. By default, no timeouts are applied. It means that the tasks will never timeout. One should use timeouts wisely and set them according to the expected type of the task. If the task, in its ordinary case, should run for no longer than 30s, you can set the timeout to 1m and keep the task from being stuck forever. Examples # 1 2 3 4 5 sergeant . config . Timeouts ( soft_timeout = 10.0 , hard_timeout = 15.0 , critical_timeout = 20.0 , )","title":"timeouts"},{"location":"worker/config/timeouts/#worker-config-timeouts","text":"The timeouts parameter controls the killer timeouts for the worker.","title":"Worker Config - timeouts"},{"location":"worker/config/timeouts/#definition","text":"1 2 3 4 5 @dataclasses . dataclass class Timeouts : soft_timeout : float = 0.0 hard_timeout : float = 0.0 critical_timeout : float = 0.0 The timeouts parameter defines how much time the process should run before the killer must try to kill it. The following timeouts can be configured: soft_timeout [both] - On serial executor, by the time this timeout is reached, a SIGINT would be sent to the worker. On threaded worker, an exception would be raised inside the thread. hard_timeout [serial] - By the time this timeout is reached, a SIGABRT would be sent to the worker. critical_timeout [serial] - By the time this timeout is reached, a SIGKILL would be sent to the worker. By default, no timeouts are applied. It means that the tasks will never timeout. One should use timeouts wisely and set them according to the expected type of the task. If the task, in its ordinary case, should run for no longer than 30s, you can set the timeout to 1m and keep the task from being stuck forever.","title":"Definition"},{"location":"worker/config/timeouts/#examples","text":"1 2 3 4 5 sergeant . config . Timeouts ( soft_timeout = 10.0 , hard_timeout = 15.0 , critical_timeout = 20.0 , )","title":"Examples"},{"location":"worker/handlers/on_failure/","text":"Worker Handler - on_failure # The on_failure handler is invoked when a task has raised an exception. The exception object will be passed to the handler. Definition # 1 2 3 4 5 def on_failure ( self , task : typing . Dict [ str , typing . Any ], exception : Exception , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on failures","title":"on_failure"},{"location":"worker/handlers/on_failure/#worker-handler-on_failure","text":"The on_failure handler is invoked when a task has raised an exception. The exception object will be passed to the handler.","title":"Worker Handler - on_failure"},{"location":"worker/handlers/on_failure/#definition","text":"1 2 3 4 5 def on_failure ( self , task : typing . Dict [ str , typing . Any ], exception : Exception , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on failures","title":"Definition"},{"location":"worker/handlers/on_max_retries/","text":"Worker Handler - on_max_retries # The on_max_retries handler is invoked when a task called the retry method more than the allowed number of times. Definition # 1 2 3 4 def on_max_retries ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_max_retries"},{"location":"worker/handlers/on_max_retries/#worker-handler-on_max_retries","text":"The on_max_retries handler is invoked when a task called the retry method more than the allowed number of times.","title":"Worker Handler - on_max_retries"},{"location":"worker/handlers/on_max_retries/#definition","text":"1 2 3 4 def on_max_retries ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_requeue/","text":"Worker Handler - on_requeue # The on_requeue handler is invoked when a task called the requeue method. Definition # 1 2 3 4 def on_requeue ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_requeue"},{"location":"worker/handlers/on_requeue/#worker-handler-on_requeue","text":"The on_requeue handler is invoked when a task called the requeue method.","title":"Worker Handler - on_requeue"},{"location":"worker/handlers/on_requeue/#definition","text":"1 2 3 4 def on_requeue ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_retry/","text":"Worker Handler - on_retry # The on_retry handler is invoked when a task called the retry method. Definition # 1 2 3 4 def on_retry ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_retry"},{"location":"worker/handlers/on_retry/#worker-handler-on_retry","text":"The on_retry handler is invoked when a task called the retry method.","title":"Worker Handler - on_retry"},{"location":"worker/handlers/on_retry/#definition","text":"1 2 3 4 def on_retry ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_success/","text":"Worker Handler - on_success # The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler. Definition # 1 2 3 4 5 def on_success ( self , task : typing . Dict [ str , typing . Any ], returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. The following use cases are possible: Fire a logging event. Implement a metrics collector.","title":"on_success"},{"location":"worker/handlers/on_success/#worker-handler-on_success","text":"The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler.","title":"Worker Handler - on_success"},{"location":"worker/handlers/on_success/#definition","text":"1 2 3 4 5 def on_success ( self , task : typing . Dict [ str , typing . Any ], returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. The following use cases are possible: Fire a logging event. Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_timeout/","text":"Worker Handler - on_timeout # The on_timeout handler is invoked when a task has timed out. Unlike other events, this event is triggered by the Killer and not by something the process has done. Definition # 1 2 3 4 def on_timeout ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on timeouts","title":"on_timeout"},{"location":"worker/handlers/on_timeout/#worker-handler-on_timeout","text":"The on_timeout handler is invoked when a task has timed out. Unlike other events, this event is triggered by the Killer and not by something the process has done.","title":"Worker Handler - on_timeout"},{"location":"worker/handlers/on_timeout/#definition","text":"1 2 3 4 def on_timeout ( self , task : typing . Dict [ str , typing . Any ], ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on timeouts","title":"Definition"},{"location":"worker/methods/apply_async_many/","text":"Worker - apply_async_many # The apply_async_many method pushes multiple tasks onto the queue in a bulk insert. Unless task_name was specified, uses the current worker name. This method is similar to apply_async_one except it gets a list of kwargs and pushes much much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The tasks will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The tasks will be pushed to the bottom of the queue. Will be pulled first. [LIFO] Definition # 1 2 3 4 5 6 def apply_async_many ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks () urls = self . extract_urls ( response . content ) self . apply_async_many ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"apply_async_many"},{"location":"worker/methods/apply_async_many/#worker-apply_async_many","text":"The apply_async_many method pushes multiple tasks onto the queue in a bulk insert. Unless task_name was specified, uses the current worker name. This method is similar to apply_async_one except it gets a list of kwargs and pushes much much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The tasks will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The tasks will be pushed to the bottom of the queue. Will be pulled first. [LIFO]","title":"Worker - apply_async_many"},{"location":"worker/methods/apply_async_many/#definition","text":"1 2 3 4 5 6 def apply_async_many ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , ) -> bool","title":"Definition"},{"location":"worker/methods/apply_async_many/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks () urls = self . extract_urls ( response . content ) self . apply_async_many ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/apply_async_one/","text":"Worker - apply_async_one # The apply_async_one method pushes a task onto the queue. Unless task_name was specified, uses the current worker name. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The task will be pushed to the bottom of the queue. Will be pulled first. [LIFO] Definition # 1 2 3 4 5 6 def apply_async_one ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks () else : self . apply_async_one ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"apply_async_one"},{"location":"worker/methods/apply_async_one/#worker-apply_async_one","text":"The apply_async_one method pushes a task onto the queue. Unless task_name was specified, uses the current worker name. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The task will be pushed to the bottom of the queue. Will be pulled first. [LIFO]","title":"Worker - apply_async_one"},{"location":"worker/methods/apply_async_one/#definition","text":"1 2 3 4 5 6 def apply_async_one ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , ) -> bool","title":"Definition"},{"location":"worker/methods/apply_async_one/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks () else : self . apply_async_one ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/get_next_tasks/","text":"Worker - get_next_tasks # The get_next_tasks method pulls number_of_tasks tasks from the queue. Unless task_name was specified, uses the current worker name. No one should use this function directly unless they know what they are doing. Definition # 1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]] Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task [ 'kwargs' ][ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . apply_async_one ( kwargs = { 'domain' : domain , 'params' : task [ 'kwargs' ][ 'params' ] }, task_name = 'filtered_task' , )","title":"get_next_tasks"},{"location":"worker/methods/get_next_tasks/#worker-get_next_tasks","text":"The get_next_tasks method pulls number_of_tasks tasks from the queue. Unless task_name was specified, uses the current worker name. No one should use this function directly unless they know what they are doing.","title":"Worker - get_next_tasks"},{"location":"worker/methods/get_next_tasks/#definition","text":"1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]]","title":"Definition"},{"location":"worker/methods/get_next_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task [ 'kwargs' ][ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . apply_async_one ( kwargs = { 'domain' : domain , 'params' : task [ 'kwargs' ][ 'params' ] }, task_name = 'filtered_task' , )","title":"Examples"},{"location":"worker/methods/number_of_enqueued_tasks/","text":"Worker - number_of_enqueued_tasks # The number_of_enqueued_tasks method return the number of all the tasks from the queue. Unless task_name was specified, uses the current worker name. Definition # 1 2 3 4 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> typing . Optional [ int ] Examples # 1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#worker-number_of_enqueued_tasks","text":"The number_of_enqueued_tasks method return the number of all the tasks from the queue. Unless task_name was specified, uses the current worker name.","title":"Worker - number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#definition","text":"1 2 3 4 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> typing . Optional [ int ]","title":"Definition"},{"location":"worker/methods/number_of_enqueued_tasks/#examples","text":"1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"Examples"},{"location":"worker/methods/purge_tasks/","text":"Worker - purge_tasks # The purge_tasks method deleted all the tasks from the queue. It allows the worker to implement some critical functionality where it identified a situation that should stop all the future tasks from being executed. Unless task_name was specified, uses the current worker name. Definition # 1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"purge_tasks"},{"location":"worker/methods/purge_tasks/#worker-purge_tasks","text":"The purge_tasks method deleted all the tasks from the queue. It allows the worker to implement some critical functionality where it identified a situation that should stop all the future tasks from being executed. Unless task_name was specified, uses the current worker name.","title":"Worker - purge_tasks"},{"location":"worker/methods/purge_tasks/#definition","text":"1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool","title":"Definition"},{"location":"worker/methods/purge_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"Examples"},{"location":"worker/methods/requeue/","text":"Worker - requeue # The requeue method pushes back the task to the queue without increasing the run count by one. The way it works is by raising a WorkerRequeue exception which cause the worker to interrupt. It means you should call requeue only if you have nothing more to do. Calling requeue and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. requeue makes the task to be considered as a failed task. Requeue from handlers One should never call requeue from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 def requeue ( self , task : typing . Dict [ str , typing . Any ], priority : str = 'NORMAL' , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"requeue"},{"location":"worker/methods/requeue/#worker-requeue","text":"The requeue method pushes back the task to the queue without increasing the run count by one. The way it works is by raising a WorkerRequeue exception which cause the worker to interrupt. It means you should call requeue only if you have nothing more to do. Calling requeue and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. requeue makes the task to be considered as a failed task. Requeue from handlers One should never call requeue from the following handlers: on_success on_retry on_max_retries on_requeue","title":"Worker - requeue"},{"location":"worker/methods/requeue/#definition","text":"1 2 3 4 5 def requeue ( self , task : typing . Dict [ str , typing . Any ], priority : str = 'NORMAL' , ) -> None","title":"Definition"},{"location":"worker/methods/requeue/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"Examples"},{"location":"worker/methods/retry/","text":"Worker - retry # The retry method pushes back the task to the queue while increasing the run count by one. The way it works is by raising a WorkerRetry exception which cause the worker to interrupt. It means you should call retry only if you have nothing more to do. Calling retry and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. retry makes the task to be considered as a failed task. Retry from handlers One should never call retry from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 def retry ( self , task : typing . Dict [ str , typing . Any ], priority : str = 'NORMAL' , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"retry"},{"location":"worker/methods/retry/#worker-retry","text":"The retry method pushes back the task to the queue while increasing the run count by one. The way it works is by raising a WorkerRetry exception which cause the worker to interrupt. It means you should call retry only if you have nothing more to do. Calling retry and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. retry makes the task to be considered as a failed task. Retry from handlers One should never call retry from the following handlers: on_success on_retry on_max_retries on_requeue","title":"Worker - retry"},{"location":"worker/methods/retry/#definition","text":"1 2 3 4 5 def retry ( self , task : typing . Dict [ str , typing . Any ], priority : str = 'NORMAL' , ) -> None","title":"Definition"},{"location":"worker/methods/retry/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"Examples"},{"location":"worker/worker/finalize/","text":"Worker - finalize # The finalize method is invoked by the once after it has exceeded the maximum number of tasks per run. This is the opportunity to collect metrics, to close handles, and to perform cleanups. Definition # 1 2 3 def finalize ( self , ) -> None Examples # 1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"finalize"},{"location":"worker/worker/finalize/#worker-finalize","text":"The finalize method is invoked by the once after it has exceeded the maximum number of tasks per run. This is the opportunity to collect metrics, to close handles, and to perform cleanups.","title":"Worker - finalize"},{"location":"worker/worker/finalize/#definition","text":"1 2 3 def finalize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/finalize/#examples","text":"1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"Examples"},{"location":"worker/worker/initialize/","text":"Worker - initialize # The initialize method is invoked by the worker once at the moment the worker is spawned by the supervisor . This method allows to implement an initialization of object that will live for the whole lifespan of the worker. A good usage example is to initialize a Logger object or an APM object. Definition # 1 2 3 def initialize ( self , ) -> None Examples # 1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"initialize"},{"location":"worker/worker/initialize/#worker-initialize","text":"The initialize method is invoked by the worker once at the moment the worker is spawned by the supervisor . This method allows to implement an initialization of object that will live for the whole lifespan of the worker. A good usage example is to initialize a Logger object or an APM object.","title":"Worker - initialize"},{"location":"worker/worker/initialize/#definition","text":"1 2 3 def initialize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/initialize/#examples","text":"1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"Examples"},{"location":"worker/worker/post_work/","text":"Worker - post_work # The post_work method is invoked by the worker for every execution of a task, after it has finished a work run. This method allows the user to perform an operation after every task is executed. It can be used to close an APM transaction, or to send a metrics. Definition # 1 2 3 4 5 6 def post_work ( self , task : typing . Dict [ str , typing . Any ], success : bool , exception : typing . Optional [ Exception ], ) -> None Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on {task[\"kwargs\"][\"url\"]} : {time.time()}, exception: {exception} ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"post_work"},{"location":"worker/worker/post_work/#worker-post_work","text":"The post_work method is invoked by the worker for every execution of a task, after it has finished a work run. This method allows the user to perform an operation after every task is executed. It can be used to close an APM transaction, or to send a metrics.","title":"Worker - post_work"},{"location":"worker/worker/post_work/#definition","text":"1 2 3 4 5 6 def post_work ( self , task : typing . Dict [ str , typing . Any ], success : bool , exception : typing . Optional [ Exception ], ) -> None","title":"Definition"},{"location":"worker/worker/post_work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on {task[\"kwargs\"][\"url\"]} : {time.time()}, exception: {exception} ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"Examples"},{"location":"worker/worker/pre_work/","text":"Worker - pre_work # The pre_work method is invoked by the worker for every execution of a task, prior to the work method. This method allows the user to perform an operation before every task is executed. It can be used to open an APM transaction, to measure the time before a task and more. The timeouts parameter would not affect the execution of this method. The killer has no effect on this method. It means that if the user will run something infinitely here, it will stuck the worker forever. Definition # 1 2 3 4 def pre_work ( self , task : typing . Dict [ str , typing . Any ], ) -> None Examples # 1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on {task[\"kwargs\"][\"url\"]} : {time.time()}' ) self . apm_client . begin_transaction ()","title":"pre_work"},{"location":"worker/worker/pre_work/#worker-pre_work","text":"The pre_work method is invoked by the worker for every execution of a task, prior to the work method. This method allows the user to perform an operation before every task is executed. It can be used to open an APM transaction, to measure the time before a task and more. The timeouts parameter would not affect the execution of this method. The killer has no effect on this method. It means that if the user will run something infinitely here, it will stuck the worker forever.","title":"Worker - pre_work"},{"location":"worker/worker/pre_work/#definition","text":"1 2 3 4 def pre_work ( self , task : typing . Dict [ str , typing . Any ], ) -> None","title":"Definition"},{"location":"worker/worker/pre_work/#examples","text":"1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on {task[\"kwargs\"][\"url\"]} : {time.time()}' ) self . apm_client . begin_transaction ()","title":"Examples"},{"location":"worker/worker/work/","text":"Worker - work # The work method is the method that should include our work login. This is where the task should be executed. The task input parameter should include all the information for the worker to perform its logic. Many worker methods pass and get the task object so they can function properly. 1 2 3 4 5 6 task = { 'name' : task_name , 'date' : datetime . datetime . utcnow () . timestamp (), 'kwargs' : kwargs , 'run_count' : 0 , } name - This is the task name. It is also the name of the queue. date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed. Definition # 1 2 3 4 def work ( self , task : typing . Dict [ str , typing . Any ], ) -> typing . Any Examples # 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"work"},{"location":"worker/worker/work/#worker-work","text":"The work method is the method that should include our work login. This is where the task should be executed. The task input parameter should include all the information for the worker to perform its logic. Many worker methods pass and get the task object so they can function properly. 1 2 3 4 5 6 task = { 'name' : task_name , 'date' : datetime . datetime . utcnow () . timestamp (), 'kwargs' : kwargs , 'run_count' : 0 , } name - This is the task name. It is also the name of the queue. date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed.","title":"Worker - work"},{"location":"worker/worker/work/#definition","text":"1 2 3 4 def work ( self , task : typing . Dict [ str , typing . Any ], ) -> typing . Any","title":"Definition"},{"location":"worker/worker/work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task [ 'kwargs' ][ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"Examples"}]}